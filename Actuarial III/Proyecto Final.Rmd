---
title: "Proyecto Final"
author: "Erick Zarza, Edmar Trapaga."
date: "11 de mayo de 2018"
output:
  html_document: 
    df_print: paged 
    theme: cerulean
    code_folding: hide
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F, 
                      error = F,fig.align = "center",
                      fig.height = 5, fig.width = 5,
                      fig.path='figure/', cache=F,
                      cache.path='cache/')
```

```{r directory_libraries_seed}
setwd("C:/Users/EZ/Desktop/ITAM/E-E/Actuarial III")

libraries <- c("tidyverse","kableExtra", "grid", "knitr", "gridExtra", "readxl", "latex2exp")

invisible(lapply(libraries, library,
                 character.only = TRUE)) #carga todos los paquete requeridos sin imprimir el resultado de hacerlo

set.seed(139380)
```

# {.tabset .tabset-fade .tabset-pills} 

1.1 Modelo Colectivo

```{r, cache=F}
# f_n <- función de masa de probabilidad de la frecuencia 
sop <- seq(0,100000,10000) #1000
lambda <- 0.1 # =E[N]= media de la frecuencia
n <- 0:5 # # soporte de la frecuencia
theta <- 10000 # =E[X]= media de la severidad


f_n <- matrix(nrow=2,ncol=length(n)) %>% as.data.frame()
for (i in 0:(length(n)-1)){
  f_n[,i+1] <- c(n[i+1],dpois(n[i+1],lambda)) 
}

colnames(f_n) <- c(paste0("Pr(N=",n,")"))
rownames(f_n) <- c("n","Pr(N=n)")

# F_nx <- distribución de la n-ésima convolusión

F_n <- function(n){ifelse(n==0,1,0)}

F_nx <- matrix(nrow=length(sop),ncol=length(n)) %>% as.data.frame()

for (i in 0:(length(n)-1)){
  F_nx[,i+1] <- c(F_n(n[i+1]),
            ifelse(n[i+1]==0,0,1)*pgamma(
              sop[-1],shape=n[i+1],scale=theta)) 
}

F_nx[,1] <- format(F_nx[,1], scientific = F) %>% as.numeric()
F_nx[1,] <- format(F_nx[1,], scientific = F) %>% as.numeric()

colnames(F_nx) <- c(paste0("",n,""))
rownames(F_nx) <- c(paste0("",format(sop, scientific = F),""))



# F_s distribución acumulada del modelo agregado

F_s <- NULL

F_s[1] <- sum(f_n[2,]*F_nx[1,])

for (i in 1:(length(sop))-1){
F_s[1] <- sum(f_n[2,]*F_nx[1,])
F_s[i+1] <- F_s[1] + sum(f_n[2,]*F_nx[i+1,])
}

# f_s densidad de probabilidad del modelo agregado

f_s <- NULL
f_s[1] <- F_s[1]
for(i in 1:(length(sop)-1)){
f_s[i+1] <- F_s[i+1]-F_s[i]
}

#tabla completa para modelo agregado

mod_agregado <- data.frame(s=format(sop, 
                                      scientific = F) %>% as.numeric(),
                           fs=f_s,Fs=F_s)
names(mod_agregado) <- c("s","f(s)","F(s)")
```

```{r}
kable(f_n, format.args = list(big.mark= ","))
kable(F_nx,format.args = list(big.mark = ","))
kable(mod_agregado,format.args = list(big.mark = ","))
```

```{r}
ggplot(mod_agregado,aes(x=s,y=`f(s)`)) + geom_point() + theme_bw()
ggplot(mod_agregado,aes(x=s,y=`F(s)`)) + geom_point() + theme_bw()

```

1.2

```{r}
n_sim <- 10000
#lambda <- 0.1
#theta <- 10000
sim_i <- NULL
N_cuenta <- NULL

for(i in 1:n_sim){
N <- rpois(1,lambda = lambda)
X_i <- rexp(N,1/theta)
S_i <- sum(X_i)
sim_i[i] <- S_i 
N_cuenta[i] <- N
}

#Data frame de las simulaciones y gráfica de dist empírica
sim_i <- data.frame(sim_i) 
colnames(sim_i) <- "sim"

ggplot(sim_i, aes(sim)) + stat_ecdf(geom = "step") + coord_cartesian(ylim=(c(.9,1)),xlim=c(0,100000)) + theme_bw()

#s <- mod_agregado$s %>% as.character() %>% as.numeric()
#f_s <- mod_agregado$`f(s)` %>% as.character() %>% as.numeric()

s <- mod_agregado$s 
f_s <- mod_agregado$`f(s)` 
```

#Gama trasladada

```{r}
theta <- 1/10000
E_X <- 1/theta #Media de severidad
E_X2 <- 2/(theta^2) #Segundo momento severidad
E_X3 <- 6/(theta^3) #TErcer momento severidad

alpha <- 4*lambda*(E_X2)^3/(E_X3)^2 # parámetro alpha para proximacion gama
beta <- 2*(E_X2/E_X3) # parámetro beta de aproximacion gama
x_0 <-lambda*E_X-(2*lambda*(E_X2)^2)/E_X3 # parámetro de traslación para aproximación gama

media_s <- sum(s*f_s)
mediasim_s <- sim_i$sim %>% mean()

E_S <- x_0+(alpha/beta)
E_S  # Media de Gama
mediasim_s # Media de simulaciones
media_s # Media de conv
```

# Varianza

```{r}
var_gama_s <- alpha/beta^2 #
var_sim_s <- var(sim_i$sim)

E_X2_convoluciones <- sum(s^2*f_s) #Segundo momento de la distribución de convoluciones
var_s <- E_X2_convoluciones-(media_s)^2 #Varianza de convoluciones

c(var_gama_s,sqrt(var_gama_s)) # Varianza de gama trasaladada
c(var_sim_s,sqrt(var_sim_s)) # Varianza de simulaciones
c(var_s,sqrt(var_s)) # Varianza de las convoluciones
```

# Sesgo

```{r}
sesgo_gama_s <- 2*alpha/beta^3 #sesgo(skewness)
sesgo_s <- sum((s-media_s)^3*f_s) #sesgo(skewness) convoluciones
c(sesgo_gama_s,sesgo_s)

coef_sesgo_gama_s <- sesgo_gama_s/(var_gama_s)^(3/2) #coeficiente de sesgo gama
coef_sesgo_s <- sesgo_s/(var_s)^(3/2) #coeficiente de sesgo convoluciones
c(coef_sesgo_gama_s,coef_sesgo_s)
```

# Kurtosis

```{r}
kurtosis_s <- sum((s-media_s)^4*f_s) #kurtosis convoluciones
coef_kur_s <- kurtosis_s/(var_s)^2

kurtosis_gama_s <- (6/alpha)+3

coef_kur_s
kurtosis_gama_s
```

#Percentiles

```{r}
percentiles <- c(0.25,0.50,0.75,0.95,0.99)
per_sim <- quantile(sim_i$sim,percentiles) #Percentiles de simulaciones
per_sim

#Percentiles de las convolusiones
#25
per25_s <- filter(mod_agregado,`F(s)`<0.25)
per25_s <- ifelse(dim(per25_s)[1]!=0,per25_s$s %>% max(),0)

#50
per50_s <- filter(mod_agregado,`F(s)`<0.50)
per50_s <- ifelse(dim(per50_s)[1]!=0,per50_s$s %>% max(),0)

#75
per75_s <- filter(mod_agregado,`F(s)`<0.75)
per75_s <- ifelse(dim(per75_s)[1]!=0,per75_s$s %>% max(),0)

#95
per95_s <- filter(mod_agregado,`F(s)`<0.95)
per95_s <- ifelse(dim(per95_s)[1]!=0,per95_s$s %>% max(),0)

#99
per99_s <- filter(mod_agregado,`F(s)`<0.99)
per99_s <- ifelse(dim(per99_s)[1]!=0,per99_s$s %>% max(),0)

per_s <- c(per25_s,per50_s,per75_s,per95_s,per99_s)
per_s #Percentiles de la convolución
```

## Pregunta 2

```{r}
set.seed(139380)

n_sim <- 10000
sim_serie <- NULL
sim_serie <- matrix(nrow=n_sim,ncol=2)
for(i in 1:(n_sim-1)){
sim_serie[1,1] <- rexp(1,lambda) #Tiempo 1
sim_serie[1,2] <- rexp(1,theta) #Monto 1

sim_serie[i+1,1] <- rexp(1,lambda) + sim_serie[i,1]

sim_serie[i+1,2] <- rexp(1,theta)
}


sim_serie <- data.frame(T_i=sim_serie[,1],
                        X_i=sim_serie[,2])

sim_serie <- sim_serie %>% mutate(S_t=cumsum(X_i))

ggplot(sim_serie, aes(x=T_i,y=X_i)) + geom_step() + coord_cartesian(xlim=c(0,1000))

ggplot(sim_serie, aes(x=T_i,y=S_t)) + geom_step() + coord_cartesian(xlim=c(0,100),ylim=c(0,200000))
```

2.3

```{r}
sop <- c(0:(n_sim-1))
 
real_X <- data.frame(soporte=c(0:100000),teorica=pexp(c(0:100000),theta))

#Intervalo de confianza para los cuantiles p
n <- n_sim
p <- seq(1/n_sim,1,by=1/n_sim) #Cuantiles deseado
q <- 1-p
alpha=.1
z <- qnorm(1-(alpha/2),0,1)

media_per <- n*p
var_per <- n*p*q
sd_per <- sqrt(var_per)

LI <- ifelse(floor(media_per + 0.5 - z*sd_per)<1
             ,1
             ,ifelse(floor(media_per + 0.5 - z*sd_per)>n_sim
                     ,n_sim
                     ,floor(media_per + 0.5 - z*sd_per)))
             

LS <- ifelse(ceiling(media_per + 0.5 + z*sd_per)<1
             ,1
             ,ifelse(ceiling(media_per + 0.5 + z*sd_per)>n_sim
                     ,n_sim
                     ,ceiling(media_per + 0.5 + z*sd_per)))

#se añaden los percentiles y los estadísticos de orden

sim_serie <- sim_serie %>% mutate(X_ord=sort(X_i),q=p,LI=X_ord[LI],LS=X_ord[LS])

ggplot(sim_serie) + stat_ecdf(aes(X_i))+ geom_line(data=real_X,aes(x=soporte,y=teorica),col="red") + geom_line(aes(x=LI,y=q),col="blue") + geom_line(aes(x=LS,y=q),col="blue")


#Regiones de confianza para la distribución
perc_int <- c(.25, .5 , .75, .9, .95, .99)
indices_perc <- n_sim*perc_int
tabla_perc_int <- data.frame(sim_serie[indices_perc,c("q","LI","LS")])
names(tabla_perc_int) <- c("Percentiles","Límite Inferior", "Límite Superior")

kable(tabla_perc_int)
```



2.4
```{r}
ggplot(data = sim_serie, aes(X_i))+geom_histogram()
```

```{r}
#Valor esperado limitado
#Limited Expected Value
min_ux <- function(x,u){y <- ifelse(x<u,x,u)}
x <- NULL
LEV <- NULL

for(i in 1:n_sim){
x <- min_ux(sim_serie$X_ord,
            sim_serie$X_ord[i])
LEV[i] <- mean(x)
}

sim_serie <- sim_serie %>% mutate(LEV)

ggplot(sim_serie) + geom_line(aes(x=X_ord,y=LEV))
```

```{r}
#Media en exceso de 
#Mean Excess Loss
Fn_x <- ecdf(sim_serie$X_i)(sim_serie$X_ord) 
Sn_x <- 1-Fn_x

sim_serie <- sim_serie %>% mutate(MEL=(mean(X_i)-LEV)/Sn_x)

ggplot(sim_serie) + geom_line(aes(x=X_ord,y=MEL))
```

```{r}
#Fuerza de mortalidad
sim_serie <- sim_serie %>% mutate(mu_x=(1/n_sim)/Sn_x)

sim_serie$mu_x[n_sim] <- 1

ggplot(sim_serie) + geom_line(aes(x=X_ord,y=mu_x))
```


```{r}
# Prueba Xi cuadrada
#muestra <- sampling::srswor(1000,length(sim_serie$X_i))

#muestraF <- sim_serie$X_i[which(muestra==1)] %>% as.data.frame()

prob_exp <- dexp(sim_serie$X_i, theta)
chisq.test(sim_serie$X_i, p=prob_exp, rescale.p = TRUE)
```

2.7
```{r}
ggplot(sim_serie, aes(sample=X_i))+geom_qq(distribution = qexp)

# El modelo parece que apenas subestima los eventos de monto alto, pero en general ajusta bastante bien. Esto es obvio, dado que los datos fueron simulados a partir de una exponencial.
```

2.8

```{r}
sop_frec <- floor(max(sim_serie$T_i))
tabla_frec <- data.frame(n=rep(0,sop_frec))
#ti <- sim_serie$T_i
#x <- NULL
#x <- matrix(nrow=21,ncol=7) 
#for(i in c(0:20)){ #corre todo los valores del soprte deseados
#  for(j in 1:5){ #evalua cada uno de los puntos de la muestra y encuentra el intervalo al que pertencece
#  x[i+1,j] <- ifelse(i<ti[j]&ti[j]<i+1,1,0)
#  x[i+1,7] <- i+1
#  }}
#for (i in 0:20){ #suma las columnas para contabilizar el total de eventos en el periodo i-ésimo
#  x[i+1,6] <- sum(x[i+1,c(1:5)])
#}

ti <- sim_serie$T_i
ceil_ti <- ceiling(ti) #techo
un_n <- unique(ceil_ti) #buscar valores único para indexarlos de un vector lleno de ceros
tab_ti <- table(ceil_ti) %>% as.data.frame() #cuenta las proporciones de los valores en la muestra
frec_n <- rep(0,sop_frec)
frec_n[un_n] <- tab_ti$Freq #llena las frecuencias para los valores que se encuentran en la muestra


n_ti <- data.frame(n=c(0:sop_frec+1),frec=frec_n)

#x <- matrix(nrow=sop_frec,ncol=n_sim+2)
#ti <- sim_serie$T_i
#for(i in 0:(sop_frec-1)){
#  for(j in 1:n_sim){
#  x[i+1,j] <- ifelse(i<ti[j]&ti[j]<i+1,1,0)
#  x[i+1,n_sim+1] <- i+1
#  }}
#for (i in 1:(sop_frec-1)){
#  x[i+1,n_sim+2] <- sum(x[i+1,])
#}

#Ajusta la distribcuión de maxima verosimilitud 
lambda_est <- MASS::fitdistr(n_ti$frec, densfun = 'Poisson')
alpha <- 0.10
z_alpha <- qnorm(1-(alpha/2),0,1)
Int_lambda <- c(lambda_est$estimate-z_alpha*lambda_est$sd
  ,lambda_est$estimate+z_alpha*lambda_est$sd)
```

Un intervalo de confianza al 90% de confianza es `r Int_lambda`

### Ejercicio 3
```{r}
Ind <- NULL
u <- 30000 # Límite de cobertura
d <- 5000 # Deducible
for(i in 1:length(sim_serie$X_i)) {
  Ind[i] <- min(sim_serie$X_i[i], u)
  Ind[i] <- max(sim_serie$X_i[i]-d,0)
  Ind[i] <- min(Ind[i], u)
}

indem_sim <- data.frame(S=sim_serie$X_i, "Indemnizaciones"=Ind)

sim_serie <- sim_serie %>% mutate(Indemnizaciones=Ind) %>% select(c(1:2,11,3:10))


# Proporción de indemnizaciones realizables de datos originales 
prop <- sum(sim_serie$Indemnizaciones!=0)/n_sim

sim_rest <- n_sim-sum(sim_serie$Indemnizaciones==0) 
# Checar formulas para saber si tendría esta proporción de manera analítica

media_indem <- sim_serie$Indemnizaciones %>% mean()
media_teorica_indem <- exp(-5000*theta)*(8008.5) + 30000*(exp(-35000*theta))

prop_indemnizable_teo <- (pexp(35000,theta)-pexp(5000,theta))

```

```{r}
kable(sim_serie[1:10,1:3])
```


Se tiene `r prop` de los datos originales una vez realizada la selección de los eventos indemnizables de las simulaciones generadas, para estimar esta proporción teóricamente calculamos la probabibilidad de pertencer al rango indemnizable , con ello la proporción indemnizable teóricamente es: $F_X(35,000)-F_X(50,000)$ = `r prop_indemnizable_teo`

 Además el valor de medio de las indemnizaciones es `r media_indem`. 
 
Para calcular el valor teórico indemnizable se calculó la media de la siguiente variable aleatoria:

$$Y=\begin{cases}
0 & \mbox{si } X<5000 \\
X-5000 & \mbox{si } 5000 \le X < 35000 \\
30000 & \mbox{si } 35000 \le X \\
\end{cases}
$$

En donde $X$ es la variable aleatoria de la severidad ($X \sim exp({\theta}=10,000)$) con lo cual $E[Y]$ = `r media_teorica_indem`.

Se observa que tanto la estimación de la proporción y del valor esperado de las indemnizaciones son muy similares a lo que se observó en las simulaciones. Con lo cuál se comprubeba la veracidad el modelo.

3.2

```{r}
#write.csv(indem_sim,
#          "./muestra_indemnizaciones.csv")

# Con los datos en excel y usando la función Solver
#n_sim*lambda #número esperado de siniestros en 100k simulaciones
#n_sim-n_sim*lambda#proporción estimada con lambda
```

```{r}
frec_indem <- sim_serie %>% filter(Indemnizaciones!=0)
sop_frec_indem <- floor(max(frec_indem$T_i))
tabla_frec_indem <- data.frame(n=rep(0,sop_frec_indem))

ti_indem <- frec_indem$T_i
ceil_ti_indem <- ceiling(ti_indem) #techo
un_n_indem <- unique(ceil_ti_indem) #buscar valores único para indexarlos de un vector lleno de ceros
tab_ti_indem <- table(ceil_ti_indem) %>% as.data.frame() #cuenta las proporciones de los valores en la muestra
frec_n_indem <- rep(0,sop_frec_indem)
frec_n[un_n_indem] <- tab_ti_indem$Freq #llena las frecuencias para los valores que se encuentran en la muestra


n_ti_indem <- data.frame(n=c(0:sop_frec+1),frec=frec_n)

#Ajusta la distribcuión de maxima verosimilitud 
lambda_est_indem <- MASS::fitdistr(n_ti_indem$frec, densfun = 'Poisson')
alpha <- 0.10
z_alpha <- qnorm(1-(alpha/2),0,1)
Int_lambda_indem <- c(lambda_est_indem$estimate-z_alpha*lambda_est_indem$sd
  ,lambda_est_indem$estimate+z_alpha*lambda_est_indem$sd)
#Int_lambda_indem
```

Un intervalo de confianza al 90% de confianza para el lambda del proceso de indemnizaciones es es `r Int_lambda_indem`, y el valor puntual estimado es $\lambda$= `r lambda_est_indem$estimate`

Con los datos en Excel y usando la función Solver, se tienen los siguientes resultados:

```{r}
n_ind <- 5976 # Observaciones totales quitando las que se tenían con indemnización 0 (debajo del deducible)
lambda_maxver_ind <- 9579.89 # Lamda estimada por máxima verosimilitud con censura (u=30,000) y truncamiento (d=5,000)
```

```{r}
#Ajusta la distribcuión de maxima verosimilitud para las indeminzaciones
theta_indem <- MASS::fitdistr(frec_indem$Indemnizaciones, densfun = 'exponential')
#theta_indem$estimate
alpha <- 0.10
z_alpha <- qnorm(1-(alpha/2),0,1)
Int_lambda <- c(lambda_est$estimate-z_alpha*lambda_est$sd
  ,lambda_est$estimate+z_alpha*lambda_est$sd)
```

El estimador máximo verosimil para las indemnizaciones es: $\theta$=`r 1/theta_indem$estimate`en forma escalar

3.3

```{r}
#Regiones de confianza
#n_sim #Número de simulaciones
indem_sim <- indem_sim %>% mutate(Ind_ord=sort(Indemnizaciones))
medias_per_ind <- n_sim* .95
vars_per_ind <- n_sim*.95*(1-.95)
desvest_per_ind <- sqrt(vars_per_ind)
lim_inf_ind <- floor(-1.96*desvest_per_ind+medias_per_ind+.5)
lim_sup_ind <- ceiling(1.96*desvest_per_ind+medias_per_ind+.5)
perc_int <- 0.95

lim_inf_ind <- c(indem_sim$Ind_ord[lim_inf_ind],tabla_perc_int[5,2])
lim_sup_ind <- c(indem_sim$Ind_ord[lim_sup_ind],tabla_perc_int[5,3])

conf_int_ind <- data.frame("Intervalos"=c("Indemnizaciones", "Severidad"),"Límite Inferior"=lim_inf_ind, "Límite Superior"=lim_sup_ind) 
conf_int_ind <- conf_int_ind %>% mutate(Cuantil_95=c(quantile(indem_sim$Indemnizaciones,.95),quantile(indem_sim$S,.95)))
kable(conf_int_ind)
```
